{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os,sys\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.image import img_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def load_image(infilename):\n",
    "    data = mpimg.imread(infilename)\n",
    "    return data\n",
    "\n",
    "def img_crop(im, w, h):\n",
    "    list_patches = []\n",
    "    imgwidth = im.shape[0]\n",
    "    imgheight = im.shape[1]\n",
    "    is_2d = len(im.shape) < 3\n",
    "    for i in range(0,imgheight,h):\n",
    "        for j in range(0,imgwidth,w):\n",
    "            if is_2d:\n",
    "                im_patch = im[j:j+w, i:i+h]\n",
    "            else:\n",
    "                im_patch = im[j:j+w, i:i+h, :]\n",
    "            list_patches.append(im_patch)\n",
    "    return list_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 100 images\n",
      "satImage_054.png\n",
      "Loading 100 images\n",
      "satImage_054.png\n"
     ]
    }
   ],
   "source": [
    "# Loaded a set of images\n",
    "root_dir = \"../dataset/training/\"\n",
    "\n",
    "image_dir = root_dir + \"images/\"\n",
    "files = os.listdir(image_dir)\n",
    "n = len(files)\n",
    "print(\"Loading \" + str(n) + \" images\")\n",
    "imgs = [load_image(image_dir + files[i]) for i in range(n)]\n",
    "print(files[0])\n",
    "\n",
    "gt_dir = root_dir + \"groundtruth/\"\n",
    "print(\"Loading \" + str(n) + \" images\")\n",
    "gt_imgs = [load_image(gt_dir + files[i]) for i in range(n)]\n",
    "print(files[0])\n",
    "\n",
    "X_train = imgs\n",
    "Y_train = gt_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract patches from a given image\n",
    "def img_crop(im, w, h):\n",
    "    list_patches = []\n",
    "    imgwidth = im.shape[0]\n",
    "    imgheight = im.shape[1]\n",
    "    is_2d = len(im.shape) < 3\n",
    "    for i in range(0,imgheight,h):\n",
    "        for j in range(0,imgwidth,w):\n",
    "            if is_2d:\n",
    "                im_patch = im[j:j+w, i:i+h]\n",
    "            else:\n",
    "                im_patch = im[j:j+w, i:i+h, :]\n",
    "            list_patches.append(im_patch)\n",
    "    return list_patches\n",
    "\n",
    "def extract_data(filename, num_images):\n",
    "    \"\"\"Extract the images into a 4D tensor [image index, y, x, channels].\n",
    "    Values are rescaled from [0, 255] down to [-0.5, 0.5].\n",
    "    \"\"\"\n",
    "    imgs = []\n",
    "    for i in range(1, num_images+1):\n",
    "        imageid = \"satImage_%.3d\" % i\n",
    "        image_filename = filename + imageid + \".png\"\n",
    "        if os.path.isfile(image_filename):\n",
    "            print ('Loading ' + image_filename)\n",
    "            img = mpimg.imread(image_filename)\n",
    "            imgs.append(img)\n",
    "        else:\n",
    "            print ('File ' + image_filename + ' does not exist')\n",
    "\n",
    "    num_images = len(imgs)\n",
    "    IMG_WIDTH = imgs[0].shape[0]\n",
    "    IMG_HEIGHT = imgs[0].shape[1]\n",
    "    N_PATCHES_PER_IMAGE = (IMG_WIDTH/IMG_PATCH_SIZE)*(IMG_HEIGHT/IMG_PATCH_SIZE)\n",
    "\n",
    "    img_patches = [img_crop(imgs[i], IMG_PATCH_SIZE, IMG_PATCH_SIZE) for i in range(num_images)]\n",
    "    data = [img_patches[i][j] for i in range(len(img_patches)) for j in range(len(img_patches[i]))]\n",
    "\n",
    "    return numpy.asarray(data)\n",
    "        \n",
    "# Assign a label to a patch v\n",
    "def value_to_class(v):\n",
    "    foreground_threshold = 0.25 # percentage of pixels > 1 required to assign a foreground label to a patch\n",
    "    df = numpy.sum(v)\n",
    "    if df > foreground_threshold:\n",
    "        return [0, 1]\n",
    "    else:\n",
    "        return [1, 0]\n",
    "\n",
    "# Extract label images\n",
    "def extract_labels(filename, num_images):\n",
    "    \"\"\"Extract the labels into a 1-hot matrix [image index, label index].\"\"\"\n",
    "    gt_imgs = []\n",
    "    for i in range(1, num_images+1):\n",
    "        imageid = \"satImage_%.3d\" % i\n",
    "        image_filename = filename + imageid + \".png\"\n",
    "        if os.path.isfile(image_filename):\n",
    "            print ('Loading ' + image_filename)\n",
    "            img = mpimg.imread(image_filename)\n",
    "            gt_imgs.append(img)\n",
    "        else:\n",
    "            print ('File ' + image_filename + ' does not exist')\n",
    "\n",
    "    num_images = len(gt_imgs)\n",
    "    gt_patches = [img_crop(gt_imgs[i], IMG_PATCH_SIZE, IMG_PATCH_SIZE) for i in range(num_images)]\n",
    "    data = numpy.asarray([gt_patches[i][j] for i in range(len(gt_patches)) for j in range(len(gt_patches[i]))])\n",
    "    labels = numpy.asarray([value_to_class(numpy.mean(data[i])) for i in range(len(data))])\n",
    "\n",
    "    # Convert to dense 1-hot representation.\n",
    "    return labels.astype(numpy.float32)\n",
    "\n",
    "\n",
    "def error_rate(predictions, labels):\n",
    "    \"\"\"Return the error rate based on dense predictions and 1-hot labels.\"\"\"\n",
    "    return 100.0 - (\n",
    "        100.0 *\n",
    "        numpy.sum(numpy.argmax(predictions, 1) == numpy.argmax(labels, 1)) /\n",
    "        predictions.shape[0])\n",
    "\n",
    "# Write predictions from neural network to a file\n",
    "def write_predictions_to_file(predictions, labels, filename):\n",
    "    max_labels = numpy.argmax(labels, 1)\n",
    "    max_predictions = numpy.argmax(predictions, 1)\n",
    "    file = open(filename, \"w\")\n",
    "    n = predictions.shape[0]\n",
    "    for i in range(0, n):\n",
    "        file.write(max_labels(i) + ' ' + max_predictions(i))\n",
    "    file.close()\n",
    "\n",
    "# Print predictions from neural network\n",
    "def print_predictions(predictions, labels):\n",
    "    max_labels = numpy.argmax(labels, 1)\n",
    "    max_predictions = numpy.argmax(predictions, 1)\n",
    "    print (str(max_labels) + ' ' + str(max_predictions))\n",
    "\n",
    "# Convert array of labels to an image\n",
    "def label_to_img(imgwidth, imgheight, w, h, labels):\n",
    "    array_labels = numpy.zeros([imgwidth, imgheight])\n",
    "    idx = 0\n",
    "    for i in range(0,imgheight,h):\n",
    "        for j in range(0,imgwidth,w):\n",
    "            if labels[idx][0] > 0.5:\n",
    "                l = 1\n",
    "            else:\n",
    "                l = 0\n",
    "            array_labels[j:j+w, i:i+h] = l\n",
    "            idx = idx + 1\n",
    "    return array_labels\n",
    "\n",
    "def img_float_to_uint8(img):\n",
    "    rimg = img - numpy.min(img)\n",
    "    rimg = (rimg / numpy.max(rimg) * PIXEL_DEPTH).round().astype(numpy.uint8)\n",
    "    return rimg\n",
    "\n",
    "def concatenate_images(img, gt_img):\n",
    "    nChannels = len(gt_img.shape)\n",
    "    w = gt_img.shape[0]\n",
    "    h = gt_img.shape[1]\n",
    "    if nChannels == 3:\n",
    "        cimg = numpy.concatenate((img, gt_img), axis=1)\n",
    "    else:\n",
    "        gt_img_3c = numpy.zeros((w, h, 3), dtype=numpy.uint8)\n",
    "        gt_img8 = img_float_to_uint8(gt_img)          \n",
    "        gt_img_3c[:,:,0] = gt_img8\n",
    "        gt_img_3c[:,:,1] = gt_img8\n",
    "        gt_img_3c[:,:,2] = gt_img8\n",
    "        img8 = img_float_to_uint8(img)\n",
    "        cimg = numpy.concatenate((img8, gt_img_3c), axis=1)\n",
    "    return cimg\n",
    "\n",
    "def make_img_overlay(img, predicted_img):\n",
    "    w = img.shape[0]\n",
    "    h = img.shape[1]\n",
    "    color_mask = numpy.zeros((w, h, 3), dtype=numpy.uint8)\n",
    "    color_mask[:,:,0] = predicted_img*PIXEL_DEPTH\n",
    "\n",
    "    img8 = img_float_to_uint8(img)\n",
    "    background = Image.fromarray(img8, 'RGB').convert(\"RGBA\")\n",
    "    overlay = Image.fromarray(color_mask, 'RGB').convert(\"RGBA\")\n",
    "    new_img = Image.blend(background, overlay, 0.2)\n",
    "    return new_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy \n",
    "\n",
    "NUM_CHANNELS = 3 # RGB images\n",
    "PIXEL_DEPTH = 255\n",
    "NUM_LABELS = 2\n",
    "TRAINING_SIZE = 20\n",
    "VALIDATION_SIZE = 5  # Size of the validation set.\n",
    "SEED = 66478  # Set to None for random seed.\n",
    "BATCH_SIZE = 16 # 64\n",
    "NUM_EPOCHS = 5\n",
    "RESTORE_MODEL = False # If True, restore existing model instead of training a new one\n",
    "RECORDING_STEP = 1000\n",
    "\n",
    "# Set image patch size in pixels\n",
    "# IMG_PATCH_SIZE should be a multiple of 4\n",
    "# image size should be an integer multiple of this number!\n",
    "IMG_PATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ../dataset/training/images/satImage_001.png\n",
      "Loading ../dataset/training/images/satImage_002.png\n",
      "Loading ../dataset/training/images/satImage_003.png\n",
      "Loading ../dataset/training/images/satImage_004.png\n",
      "Loading ../dataset/training/images/satImage_005.png\n",
      "Loading ../dataset/training/images/satImage_006.png\n",
      "Loading ../dataset/training/images/satImage_007.png\n",
      "Loading ../dataset/training/images/satImage_008.png\n",
      "Loading ../dataset/training/images/satImage_009.png\n",
      "Loading ../dataset/training/images/satImage_010.png\n",
      "Loading ../dataset/training/images/satImage_011.png\n",
      "Loading ../dataset/training/images/satImage_012.png\n",
      "Loading ../dataset/training/images/satImage_013.png\n",
      "Loading ../dataset/training/images/satImage_014.png\n",
      "Loading ../dataset/training/images/satImage_015.png\n",
      "Loading ../dataset/training/images/satImage_016.png\n",
      "Loading ../dataset/training/images/satImage_017.png\n",
      "Loading ../dataset/training/images/satImage_018.png\n",
      "Loading ../dataset/training/images/satImage_019.png\n",
      "Loading ../dataset/training/images/satImage_020.png\n",
      "Loading ../dataset/training/groundtruth/satImage_001.png\n",
      "Loading ../dataset/training/groundtruth/satImage_002.png\n",
      "Loading ../dataset/training/groundtruth/satImage_003.png\n",
      "Loading ../dataset/training/groundtruth/satImage_004.png\n",
      "Loading ../dataset/training/groundtruth/satImage_005.png\n",
      "Loading ../dataset/training/groundtruth/satImage_006.png\n",
      "Loading ../dataset/training/groundtruth/satImage_007.png\n",
      "Loading ../dataset/training/groundtruth/satImage_008.png\n",
      "Loading ../dataset/training/groundtruth/satImage_009.png\n",
      "Loading ../dataset/training/groundtruth/satImage_010.png\n",
      "Loading ../dataset/training/groundtruth/satImage_011.png\n",
      "Loading ../dataset/training/groundtruth/satImage_012.png\n",
      "Loading ../dataset/training/groundtruth/satImage_013.png\n",
      "Loading ../dataset/training/groundtruth/satImage_014.png\n",
      "Loading ../dataset/training/groundtruth/satImage_015.png\n",
      "Loading ../dataset/training/groundtruth/satImage_016.png\n",
      "Loading ../dataset/training/groundtruth/satImage_017.png\n",
      "Loading ../dataset/training/groundtruth/satImage_018.png\n",
      "Loading ../dataset/training/groundtruth/satImage_019.png\n",
      "Loading ../dataset/training/groundtruth/satImage_020.png\n"
     ]
    }
   ],
   "source": [
    "data_dir = '../dataset/training/'\n",
    "train_data_filename = data_dir + 'images/'\n",
    "train_labels_filename = data_dir + 'groundtruth/' \n",
    "\n",
    "# Extract it into numpy arrays.\n",
    "train_data = extract_data(train_data_filename, TRAINING_SIZE)\n",
    "train_labels = extract_labels(train_labels_filename, TRAINING_SIZE)\n",
    "\n",
    "num_epochs = NUM_EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12500, 16, 16, 3)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points per class: c0 = 3050 c1 = 3050\n",
      "Balancing training data...\n",
      "6100\n",
      "(6100, 16, 16, 3)\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " ...\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n",
      "Number of data points per class: c0 = 3050 c1 = 3050\n"
     ]
    }
   ],
   "source": [
    "c0 = 0\n",
    "c1 = 0\n",
    "for i in range(len(train_labels)):\n",
    "    if train_labels[i][0] == 1:\n",
    "        c0 = c0 + 1\n",
    "    else:\n",
    "        c1 = c1 + 1\n",
    "print ('Number of data points per class: c0 = ' + str(c0) + ' c1 = ' + str(c1))\n",
    "\n",
    "print ('Balancing training data...')\n",
    "min_c = min(c0, c1)\n",
    "idx0 = [i for i, j in enumerate(train_labels) if j[0] == 1]\n",
    "idx1 = [i for i, j in enumerate(train_labels) if j[1] == 1]\n",
    "new_indices = idx0[0:min_c] + idx1[0:min_c]\n",
    "print (len(new_indices))\n",
    "print (train_data.shape)\n",
    "train_data = train_data[new_indices,:,:,:]\n",
    "train_labels = train_labels[new_indices]\n",
    "\n",
    "print(train_labels)\n",
    "train_size = train_labels.shape[0]\n",
    "\n",
    "c0 = 0\n",
    "c1 = 0\n",
    "for i in range(len(train_labels)):\n",
    "    if train_labels[i][0] == 1:\n",
    "        c0 = c0 + 1\n",
    "    else:\n",
    "        c1 = c1 + 1\n",
    "print ('Number of data points per class: c0 = ' + str(c0) + ' c1 = ' + str(c1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten\n",
    "\n",
    "#create model\n",
    "model = Sequential()\n",
    "\n",
    "#add model layers\n",
    "model.add(Conv2D(16, kernel_size=16, activation='relu', input_shape=(16,16,3)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "#compile model using accuracy to measure model performance\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6100/6100 [==============================] - 0s 56us/step - loss: 0.6707 - acc: 0.5892\n",
      "Epoch 2/100\n",
      "6100/6100 [==============================] - 0s 55us/step - loss: 0.6618 - acc: 0.6010\n",
      "Epoch 3/100\n",
      "6100/6100 [==============================] - 0s 55us/step - loss: 0.6588 - acc: 0.6048\n",
      "Epoch 4/100\n",
      "6100/6100 [==============================] - 0s 55us/step - loss: 0.6521 - acc: 0.6121\n",
      "Epoch 5/100\n",
      "6100/6100 [==============================] - 0s 55us/step - loss: 0.6509 - acc: 0.6126\n",
      "Epoch 6/100\n",
      "6100/6100 [==============================] - 0s 55us/step - loss: 0.6467 - acc: 0.6220\n",
      "Epoch 7/100\n",
      "6100/6100 [==============================] - 0s 55us/step - loss: 0.6474 - acc: 0.6152\n",
      "Epoch 8/100\n",
      "6100/6100 [==============================] - 0s 54us/step - loss: 0.6448 - acc: 0.6234\n",
      "Epoch 9/100\n",
      "6100/6100 [==============================] - 0s 54us/step - loss: 0.6402 - acc: 0.6225\n",
      "Epoch 10/100\n",
      "6100/6100 [==============================] - 0s 54us/step - loss: 0.6397 - acc: 0.6282\n",
      "Epoch 11/100\n",
      "6100/6100 [==============================] - 0s 54us/step - loss: 0.6357 - acc: 0.6339\n",
      "Epoch 12/100\n",
      "6100/6100 [==============================] - 0s 55us/step - loss: 0.6357 - acc: 0.6346\n",
      "Epoch 13/100\n",
      "6100/6100 [==============================] - 0s 55us/step - loss: 0.6328 - acc: 0.6349\n",
      "Epoch 14/100\n",
      "6100/6100 [==============================] - 0s 55us/step - loss: 0.6330 - acc: 0.6397\n",
      "Epoch 15/100\n",
      "6100/6100 [==============================] - 0s 55us/step - loss: 0.6309 - acc: 0.6370\n",
      "Epoch 16/100\n",
      "6100/6100 [==============================] - 0s 55us/step - loss: 0.6296 - acc: 0.6392\n",
      "Epoch 17/100\n",
      "6100/6100 [==============================] - 0s 55us/step - loss: 0.6272 - acc: 0.6466\n",
      "Epoch 18/100\n",
      "6100/6100 [==============================] - 0s 55us/step - loss: 0.6267 - acc: 0.6448\n",
      "Epoch 19/100\n",
      "6100/6100 [==============================] - 0s 55us/step - loss: 0.6294 - acc: 0.6446\n",
      "Epoch 20/100\n",
      "6100/6100 [==============================] - 0s 55us/step - loss: 0.6264 - acc: 0.6474\n",
      "Epoch 21/100\n",
      "6100/6100 [==============================] - 0s 55us/step - loss: 0.6233 - acc: 0.6500\n",
      "Epoch 22/100\n",
      "6100/6100 [==============================] - 0s 55us/step - loss: 0.6205 - acc: 0.6548\n",
      "Epoch 23/100\n",
      "6100/6100 [==============================] - 0s 55us/step - loss: 0.6277 - acc: 0.6403\n",
      "Epoch 24/100\n",
      "6100/6100 [==============================] - 0s 55us/step - loss: 0.6193 - acc: 0.6556\n",
      "Epoch 25/100\n",
      "6100/6100 [==============================] - 0s 55us/step - loss: 0.6171 - acc: 0.6575\n",
      "Epoch 26/100\n",
      "6100/6100 [==============================] - 0s 54us/step - loss: 0.6172 - acc: 0.6534\n",
      "Epoch 27/100\n",
      "6100/6100 [==============================] - 0s 55us/step - loss: 0.6181 - acc: 0.6507\n",
      "Epoch 28/100\n",
      "6100/6100 [==============================] - 0s 55us/step - loss: 0.6170 - acc: 0.6613\n",
      "Epoch 29/100\n",
      "6100/6100 [==============================] - 0s 55us/step - loss: 0.6130 - acc: 0.6648\n",
      "Epoch 30/100\n",
      "6100/6100 [==============================] - 0s 55us/step - loss: 0.6160 - acc: 0.6590\n",
      "Epoch 31/100\n",
      "6100/6100 [==============================] - 0s 54us/step - loss: 0.6146 - acc: 0.6580\n",
      "Epoch 32/100\n",
      "6100/6100 [==============================] - 0s 54us/step - loss: 0.6094 - acc: 0.6695\n",
      "Epoch 33/100\n",
      "6100/6100 [==============================] - 0s 54us/step - loss: 0.6089 - acc: 0.6661\n",
      "Epoch 34/100\n",
      "6100/6100 [==============================] - 0s 55us/step - loss: 0.6077 - acc: 0.6679\n",
      "Epoch 35/100\n",
      "6100/6100 [==============================] - 0s 55us/step - loss: 0.6066 - acc: 0.6710\n",
      "Epoch 36/100\n",
      "6100/6100 [==============================] - 0s 54us/step - loss: 0.6056 - acc: 0.6723\n",
      "Epoch 37/100\n",
      "6100/6100 [==============================] - 0s 54us/step - loss: 0.6084 - acc: 0.6713\n",
      "Epoch 38/100\n",
      "6100/6100 [==============================] - 0s 54us/step - loss: 0.6033 - acc: 0.6728\n",
      "Epoch 39/100\n",
      "6100/6100 [==============================] - 0s 54us/step - loss: 0.6046 - acc: 0.6713\n",
      "Epoch 40/100\n",
      "6100/6100 [==============================] - 0s 54us/step - loss: 0.6036 - acc: 0.6721\n",
      "Epoch 41/100\n",
      "6100/6100 [==============================] - 0s 54us/step - loss: 0.6018 - acc: 0.6790\n",
      "Epoch 42/100\n",
      "6100/6100 [==============================] - 0s 55us/step - loss: 0.6008 - acc: 0.6730\n",
      "Epoch 43/100\n",
      "6100/6100 [==============================] - 0s 55us/step - loss: 0.6041 - acc: 0.6746\n",
      "Epoch 44/100\n",
      "6100/6100 [==============================] - 0s 55us/step - loss: 0.5999 - acc: 0.6767\n",
      "Epoch 45/100\n",
      "6100/6100 [==============================] - 0s 55us/step - loss: 0.6007 - acc: 0.6749\n",
      "Epoch 46/100\n",
      "6100/6100 [==============================] - 0s 54us/step - loss: 0.5980 - acc: 0.6795\n",
      "Epoch 47/100\n",
      "6100/6100 [==============================] - 0s 55us/step - loss: 0.5984 - acc: 0.6754\n",
      "Epoch 48/100\n",
      "6100/6100 [==============================] - 0s 55us/step - loss: 0.5988 - acc: 0.6782\n",
      "Epoch 49/100\n",
      "6100/6100 [==============================] - 0s 55us/step - loss: 0.5965 - acc: 0.6795\n",
      "Epoch 50/100\n",
      "6100/6100 [==============================] - 0s 55us/step - loss: 0.5948 - acc: 0.6852\n",
      "Epoch 51/100\n",
      "6100/6100 [==============================] - 0s 55us/step - loss: 0.5967 - acc: 0.6807\n",
      "Epoch 52/100\n",
      "6100/6100 [==============================] - 0s 55us/step - loss: 0.6029 - acc: 0.6711\n",
      "Epoch 53/100\n",
      "6100/6100 [==============================] - 0s 55us/step - loss: 0.5965 - acc: 0.6774\n",
      "Epoch 54/100\n",
      "6100/6100 [==============================] - 0s 55us/step - loss: 0.5907 - acc: 0.6852\n",
      "Epoch 55/100\n",
      "6100/6100 [==============================] - 0s 55us/step - loss: 0.5921 - acc: 0.6846\n",
      "Epoch 56/100\n",
      "6100/6100 [==============================] - 0s 54us/step - loss: 0.5907 - acc: 0.6839\n",
      "Epoch 57/100\n",
      "6100/6100 [==============================] - 0s 54us/step - loss: 0.5931 - acc: 0.6818\n",
      "Epoch 58/100\n",
      "6100/6100 [==============================] - 0s 68us/step - loss: 0.5940 - acc: 0.6833\n",
      "Epoch 59/100\n",
      "6100/6100 [==============================] - 0s 73us/step - loss: 0.5913 - acc: 0.6813\n",
      "Epoch 60/100\n",
      "6100/6100 [==============================] - 0s 71us/step - loss: 0.5907 - acc: 0.6880\n",
      "Epoch 61/100\n",
      "6100/6100 [==============================] - 0s 73us/step - loss: 0.5888 - acc: 0.6885\n",
      "Epoch 62/100\n",
      "6100/6100 [==============================] - 0s 73us/step - loss: 0.5901 - acc: 0.6890\n",
      "Epoch 63/100\n",
      "6100/6100 [==============================] - 0s 74us/step - loss: 0.5895 - acc: 0.6879\n",
      "Epoch 64/100\n",
      "6100/6100 [==============================] - 0s 73us/step - loss: 0.5911 - acc: 0.6848\n",
      "Epoch 65/100\n",
      "6100/6100 [==============================] - 0s 73us/step - loss: 0.5900 - acc: 0.6874\n",
      "Epoch 66/100\n",
      "6100/6100 [==============================] - 0s 72us/step - loss: 0.5893 - acc: 0.6839\n",
      "Epoch 67/100\n",
      "6100/6100 [==============================] - 0s 73us/step - loss: 0.5935 - acc: 0.6826\n",
      "Epoch 68/100\n",
      "6100/6100 [==============================] - 0s 70us/step - loss: 0.5840 - acc: 0.6918\n",
      "Epoch 69/100\n",
      "6100/6100 [==============================] - 0s 70us/step - loss: 0.5871 - acc: 0.6869\n",
      "Epoch 70/100\n",
      "6100/6100 [==============================] - 0s 72us/step - loss: 0.5864 - acc: 0.6944\n",
      "Epoch 71/100\n",
      "6100/6100 [==============================] - 0s 72us/step - loss: 0.5813 - acc: 0.6989\n",
      "Epoch 72/100\n",
      "6100/6100 [==============================] - 0s 73us/step - loss: 0.5922 - acc: 0.6839\n",
      "Epoch 73/100\n",
      "6100/6100 [==============================] - 0s 75us/step - loss: 0.5868 - acc: 0.6843\n",
      "Epoch 74/100\n",
      "6100/6100 [==============================] - 0s 73us/step - loss: 0.5836 - acc: 0.6923\n",
      "Epoch 75/100\n",
      "6100/6100 [==============================] - 0s 70us/step - loss: 0.5888 - acc: 0.6836\n",
      "Epoch 76/100\n",
      "6100/6100 [==============================] - 0s 69us/step - loss: 0.5893 - acc: 0.6890\n",
      "Epoch 77/100\n",
      "6100/6100 [==============================] - 0s 70us/step - loss: 0.5841 - acc: 0.6941\n",
      "Epoch 78/100\n",
      "6100/6100 [==============================] - 0s 72us/step - loss: 0.5819 - acc: 0.6951\n",
      "Epoch 79/100\n",
      "6100/6100 [==============================] - 0s 75us/step - loss: 0.5819 - acc: 0.6923\n",
      "Epoch 80/100\n",
      "6100/6100 [==============================] - 0s 75us/step - loss: 0.5853 - acc: 0.6872\n",
      "Epoch 81/100\n",
      "6100/6100 [==============================] - 0s 72us/step - loss: 0.5787 - acc: 0.6969\n",
      "Epoch 82/100\n",
      "6100/6100 [==============================] - 0s 71us/step - loss: 0.5838 - acc: 0.6902\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6100/6100 [==============================] - 0s 68us/step - loss: 0.5822 - acc: 0.6952\n",
      "Epoch 84/100\n",
      "6100/6100 [==============================] - 0s 54us/step - loss: 0.5813 - acc: 0.6934\n",
      "Epoch 85/100\n",
      "6100/6100 [==============================] - 0s 54us/step - loss: 0.5805 - acc: 0.6969\n",
      "Epoch 86/100\n",
      "6100/6100 [==============================] - 0s 54us/step - loss: 0.5800 - acc: 0.6952\n",
      "Epoch 87/100\n",
      "6100/6100 [==============================] - 0s 54us/step - loss: 0.5795 - acc: 0.7003\n",
      "Epoch 88/100\n",
      "6100/6100 [==============================] - 0s 54us/step - loss: 0.5828 - acc: 0.6928\n",
      "Epoch 89/100\n",
      "6100/6100 [==============================] - 0s 54us/step - loss: 0.5797 - acc: 0.6956\n",
      "Epoch 90/100\n",
      "6100/6100 [==============================] - 0s 54us/step - loss: 0.5771 - acc: 0.7025\n",
      "Epoch 91/100\n",
      "6100/6100 [==============================] - 0s 54us/step - loss: 0.5844 - acc: 0.6911\n",
      "Epoch 92/100\n",
      "6100/6100 [==============================] - 0s 55us/step - loss: 0.5784 - acc: 0.6992\n",
      "Epoch 93/100\n",
      "6100/6100 [==============================] - 0s 55us/step - loss: 0.5804 - acc: 0.6946\n",
      "Epoch 94/100\n",
      "6100/6100 [==============================] - 0s 55us/step - loss: 0.5917 - acc: 0.6784\n",
      "Epoch 95/100\n",
      "6100/6100 [==============================] - 0s 55us/step - loss: 0.5807 - acc: 0.6915\n",
      "Epoch 96/100\n",
      "6100/6100 [==============================] - 0s 54us/step - loss: 0.5794 - acc: 0.6961\n",
      "Epoch 97/100\n",
      "6100/6100 [==============================] - 0s 54us/step - loss: 0.5787 - acc: 0.6989\n",
      "Epoch 98/100\n",
      "6100/6100 [==============================] - 0s 54us/step - loss: 0.5793 - acc: 0.6926\n",
      "Epoch 99/100\n",
      "6100/6100 [==============================] - 0s 54us/step - loss: 0.5805 - acc: 0.6944\n",
      "Epoch 100/100\n",
      "6100/6100 [==============================] - 0s 54us/step - loss: 0.5814 - acc: 0.6928\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f336e9a74a8>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data, train_labels, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6100/6100 [==============================] - 0s 10us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.697208758416723, 0.47885245903837875]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = model.evaluate(train_data, train_labels, batch_size=128)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get prediction for given input image \n",
    "def get_prediction(img):\n",
    "    data = numpy.asarray(img_crop(img, IMG_PATCH_SIZE, IMG_PATCH_SIZE))\n",
    "    output = model.predict(data)\n",
    "    output_prediction = output\n",
    "    img_prediction = label_to_img(img.shape[0], img.shape[1], IMG_PATCH_SIZE, IMG_PATCH_SIZE, output_prediction)\n",
    "\n",
    "    return img_prediction\n",
    "\n",
    "# Get a concatenation of the prediction and groundtruth for given input file\n",
    "def get_prediction_with_groundtruth(filename, image_idx):\n",
    "\n",
    "    imageid = \"satImage_%.3d\" % image_idx\n",
    "    image_filename = filename + imageid + \".png\"\n",
    "    img = mpimg.imread(image_filename)\n",
    "\n",
    "    img_prediction = get_prediction(img)\n",
    "    cimg = concatenate_images(img, img_prediction)\n",
    "\n",
    "    return cimg\n",
    "\n",
    "# Get prediction overlaid on the original image for given input file\n",
    "def get_prediction_with_overlay(filename, image_idx):\n",
    "\n",
    "    imageid = \"satImage_%.3d\" % image_idx\n",
    "    image_filename = filename + imageid + \".png\"\n",
    "    img = mpimg.imread(image_filename)\n",
    "\n",
    "    img_prediction = get_prediction(img)\n",
    "    oimg = make_img_overlay(img, img_prediction)\n",
    "\n",
    "    return oimg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running prediction on training set\n"
     ]
    }
   ],
   "source": [
    "print (\"Running prediction on training set\")\n",
    "prediction_training_dir = \"predictions_training/\"\n",
    "if not os.path.isdir(prediction_training_dir):\n",
    "    os.mkdir(prediction_training_dir)\n",
    "for i in range(1, TRAINING_SIZE+1):\n",
    "    pimg = get_prediction_with_groundtruth(train_data_filename, i)\n",
    "    Image.fromarray(pimg).save(prediction_training_dir + \"prediction_\" + str(i) + \".png\")\n",
    "    oimg = get_prediction_with_overlay(train_data_filename, i)\n",
    "    oimg.save(prediction_training_dir + \"overlay_\" + str(i) + \".png\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_generator():\n",
    "    while True:\n",
    "        for start in range(0, len(ids_train_split), batch_size):\n",
    "            x_batch = []\n",
    "            y_batch = []\n",
    "            end = min(start + batch_size, len(ids_train_split))\n",
    "            ids_train_batch = ids_train_split[start:end]\n",
    "            for id in ids_train_batch.values:\n",
    "                img = cv2.imread('input/train/{}.jpg'.format(id))\n",
    "                img = cv2.resize(img, (input_size, input_size))\n",
    "                mask = cv2.imread('input/train_masks/{}_mask.png'.format(id), cv2.IMREAD_GRAYSCALE)\n",
    "                mask = cv2.resize(mask, (input_size, input_size))\n",
    "                img = randomHueSaturationValue(img,\n",
    "                                               hue_shift_limit=(-50, 50),\n",
    "                                               sat_shift_limit=(-5, 5),\n",
    "                                               val_shift_limit=(-15, 15))\n",
    "                img, mask = randomShiftScaleRotate(img, mask,\n",
    "                                                   shift_limit=(-0.0625, 0.0625),\n",
    "                                                   scale_limit=(-0.1, 0.1),\n",
    "                                                   rotate_limit=(-0, 0))\n",
    "                img, mask = randomHorizontalFlip(img, mask)\n",
    "                mask = np.expand_dims(mask, axis=2)\n",
    "                x_batch.append(img)\n",
    "                y_batch.append(mask)\n",
    "            x_batch = np.array(x_batch, np.float32) / 255\n",
    "            y_batch = np.array(y_batch, np.float32) / 255\n",
    "            yield x_batch, y_batch\n",
    "\n",
    "\n",
    "def valid_generator():\n",
    "    while True:\n",
    "        for start in range(0, len(ids_valid_split), batch_size):\n",
    "            x_batch = []\n",
    "            y_batch = []\n",
    "            end = min(start + batch_size, len(ids_valid_split))\n",
    "            ids_valid_batch = ids_valid_split[start:end]\n",
    "            for id in ids_valid_batch.values:\n",
    "                img = cv2.imread('input/train/{}.jpg'.format(id))\n",
    "                img = cv2.resize(img, (input_size, input_size))\n",
    "                mask = cv2.imread('input/train_masks/{}_mask.png'.format(id), cv2.IMREAD_GRAYSCALE)\n",
    "                mask = cv2.resize(mask, (input_size, input_size))\n",
    "                mask = np.expand_dims(mask, axis=2)\n",
    "                x_batch.append(img)\n",
    "                y_batch.append(mask)\n",
    "            x_batch = np.array(x_batch, np.float32) / 255\n",
    "            y_batch = np.array(y_batch, np.float32) / 255\n",
    "            yield x_batch, y_batch\n",
    "\n",
    "\n",
    "callbacks = [EarlyStopping(monitor='val_loss',\n",
    "                           patience=8,\n",
    "                           verbose=1,\n",
    "                           min_delta=1e-4),\n",
    "             ReduceLROnPlateau(monitor='val_loss',\n",
    "                               factor=0.1,\n",
    "                               patience=4,\n",
    "                               verbose=1,\n",
    "                               epsilon=1e-4),\n",
    "             ModelCheckpoint(monitor='val_loss',\n",
    "                             filepath='weights/best_weights.hdf5',\n",
    "                             save_best_only=True,\n",
    "                             save_weights_only=True),\n",
    "             TensorBoard(log_dir='logs')]\n",
    "\n",
    "model.fit_generator(generator=train_generator(),\n",
    "                    steps_per_epoch=np.ceil(float(len(ids_train_split)) / float(batch_size)),\n",
    "                    epochs=epochs,\n",
    "                    verbose=2,\n",
    "                    callbacks=callbacks,\n",
    "                    validation_data=valid_generator(),\n",
    "                    validation_steps=np.ceil(float(len(ids_valid_split)) / float(batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert array of labels to an image\n",
    "\n",
    "def label_to_img(imgwidth, imgheight, w, h, labels):\n",
    "    im = np.zeros([imgwidth, imgheight])\n",
    "    idx = 0\n",
    "    for i in range(0,imgheight,h):\n",
    "        for j in range(0,imgwidth,w):\n",
    "            im[j:j+w, i:i+h] = labels[idx]\n",
    "            idx = idx + 1\n",
    "    return im\n",
    "\n",
    "def make_img_overlay(img, predicted_img):\n",
    "    w = img.shape[0]\n",
    "    h = img.shape[1]\n",
    "    color_mask = np.zeros((w, h, 3), dtype=np.uint8)\n",
    "    color_mask[:,:,0] = predicted_img*255\n",
    "\n",
    "    img8 = img_float_to_uint8(img)\n",
    "    background = Image.fromarray(img8, 'RGB').convert(\"RGBA\")\n",
    "    overlay = Image.fromarray(color_mask, 'RGB').convert(\"RGBA\")\n",
    "    new_img = Image.blend(background, overlay, 0.2)\n",
    "    return new_img\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
